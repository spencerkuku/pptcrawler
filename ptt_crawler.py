import requests
from bs4 import BeautifulSoup
import re
import json
import time
import os
import sys
import argparse
import concurrent.futures
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Union
import logging
from urllib.parse import urljoin
import yaml

# é€²åº¦æ¢åº«
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("å»ºè­°å®‰è£ tqdm ä¾†é¡¯ç¤ºé€²åº¦æ¢: pip install tqdm")

# pandas for CSV export
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

@dataclass
class Article:
    """æ–‡ç« æ•¸æ“šçµæ§‹"""
    board: str
    article_id: str
    title: str
    author: str
    date: str
    content: str
    url: str
    ip: str = "Unknown"
    push_count: int = 0
    boo_count: int = 0
    neutral_count: int = 0
    total_messages: int = 0
    messages: List[Dict] = None
    crawl_time: str = ""
    
    def __post_init__(self):
        if self.messages is None:
            self.messages = []
        if not self.crawl_time:
            self.crawl_time = datetime.now().isoformat()

@dataclass
class CrawlConfig:
    """çˆ¬èŸ²é…ç½®"""
    delay_between_requests: float = 0.1
    delay_between_pages: float = 0.5
    timeout: int = 10
    max_retries: int = 3
    max_workers: int = 4
    output_dir: str = "./crawled_data"
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    
    @classmethod
    def from_file(cls, config_path: str) -> 'CrawlConfig':
        """å¾é…ç½®æ–‡ä»¶è¼‰å…¥"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
                return cls(**config_data)
        return cls()
    
    def save_to_file(self, config_path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(asdict(self), f, default_flow_style=False, allow_unicode=True)

class PTTCrawler:
    """PTT çˆ¬èŸ²ä¸»é¡åˆ¥"""
    
    def __init__(self, config: CrawlConfig = None):
        self.config = config or CrawlConfig()
        self.session = self._create_session()
        self.logger = self._setup_logger()
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
    
    def _create_session(self) -> requests.Session:
        """å‰µå»ºè«‹æ±‚æœƒè©±"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self.config.user_agent
        })
        session.cookies.update({'over18': '1'})  # å¹´é½¡é©—è­‰
        return session
    
    def _setup_logger(self) -> logging.Logger:
        """è¨­ç½®æ—¥èªŒ"""
        logger = logging.getLogger('PTTCrawler')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _make_request(self, url: str, retries: int = None) -> Optional[requests.Response]:
        """å¸¶é‡è©¦æ©Ÿåˆ¶çš„è«‹æ±‚"""
        max_retries = retries or self.config.max_retries
        
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=self.config.timeout)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                self.logger.warning(f"è«‹æ±‚å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                else:
                    self.logger.error(f"è«‹æ±‚æœ€çµ‚å¤±æ•—: {url}")
                    return None
    
    def get_latest_page_number(self, board_name: str) -> int:
        """ç²å–çœ‹æ¿æœ€æ–°é æ•¸"""
        url = f"https://www.ptt.cc/bbs/{board_name}/index.html"
        response = self._make_request(url)
        
        if not response:
            return 0
        
        # æ–¹æ³•1: æŸ¥æ‰¾ä¸Šä¸€é é€£çµ
        pattern = rf'href="/bbs/{board_name}/index(\d+)\.html">&lsaquo;'
        match = re.search(pattern, response.text)
        if match:
            return int(match.group(1)) + 1
        
        # æ–¹æ³•2: è§£ææ‰€æœ‰é æ•¸é€£çµ
        soup = BeautifulSoup(response.text, 'html.parser')
        page_numbers = soup.find_all('a', href=re.compile(rf'/{board_name}/index(\d+)\.html'))
        
        if page_numbers:
            pages = [int(p.get('href').split('index')[1].replace('.html', '')) 
                    for p in page_numbers]
            return max(pages)
        
        return 0
    
    def extract_articles_from_page(self, board_name: str, page_num: int) -> List[Dict]:
        """å¾æŒ‡å®šé é¢æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯"""
        url = f"https://www.ptt.cc/bbs/{board_name}/index{page_num}.html"
        response = self._make_request(url)
        
        if not response:
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = []
        
        for div in soup.find_all("div", class_="r-ent"):
            link_elem = div.find('a')
            if not link_elem or not link_elem.get('href'):
                continue
            
            href = link_elem['href']
            article_url = urljoin("https://www.ptt.cc", href)
            article_id = href.split('/')[-1].replace('.html', '')
            title = link_elem.get_text(strip=True)
            
            # æå–ä½œè€…å’Œæ—¥æœŸ
            author_elem = div.find('div', class_='author')
            date_elem = div.find('div', class_='date')
            
            # æå–æ¨æ–‡æ•¸
            push_elem = div.find('div', class_='nrec')
            push_text = push_elem.get_text(strip=True) if push_elem else ''
            
            articles.append({
                'board': board_name,
                'article_id': article_id,
                'title': title,
                'author': author_elem.get_text(strip=True) if author_elem else '',
                'date': date_elem.get_text(strip=True) if date_elem else '',
                'url': article_url,
                'push_preview': push_text
            })
        
        return articles
    
    def parse_article_content(self, article_url: str) -> Optional[Dict]:
        """è§£æå–®ç¯‡æ–‡ç« è©³ç´°å…§å®¹"""
        response = self._make_request(article_url)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        main_content = soup.find(id="main-content")
        
        if not main_content:
            self.logger.warning(f"æ‰¾ä¸åˆ°æ–‡ç« å…§å®¹: {article_url}")
            return None
        
        # è§£æ metadata
        metas = main_content.select('div.article-metaline')
        author = title = date = ''
        
        if len(metas) >= 3:
            try:
                author = metas[0].select('span.article-meta-value')[0].get_text(strip=True)
                title = metas[1].select('span.article-meta-value')[0].get_text(strip=True)
                date = metas[2].select('span.article-meta-value')[0].get_text(strip=True)
            except (IndexError, AttributeError):
                pass
        
        # ç§»é™¤ metadata
        for meta in metas:
            meta.extract()
        for meta in main_content.select('div.article-metaline-right'):
            meta.extract()
        
        # è§£ææ¨æ–‡
        pushes = main_content.find_all('div', class_='push')
        messages = []
        push_count = boo_count = neutral_count = 0
        
        for push in pushes:
            push.extract()
            
            try:
                tag_elem = push.find('span', 'push-tag')
                userid_elem = push.find('span', 'push-userid')
                content_elem = push.find('span', 'push-content')
                datetime_elem = push.find('span', 'push-ipdatetime')
                
                if all([tag_elem, userid_elem, content_elem, datetime_elem]):
                    tag = tag_elem.get_text(strip=True)
                    userid = userid_elem.get_text(strip=True)
                    content = content_elem.get_text(strip=True)
                    datetime_str = datetime_elem.get_text(strip=True)
                    
                    if content.startswith(':'):
                        content = content[1:].strip()
                    
                    # è§£æ IP å’Œæ™‚é–“
                    ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
                    ip_match = re.search(ip_pattern, datetime_str)
                    
                    if ip_match:
                        ip = ip_match.group()
                        datetime_clean = datetime_str.replace(ip, '').strip()
                    else:
                        ip = "Unknown"
                        datetime_clean = datetime_str
                    
                    messages.append({
                        'push_tag': tag,
                        'push_userid': userid,
                        'push_content': content,
                        'push_ip': ip,
                        'push_datetime': datetime_clean
                    })
                    
                    # çµ±è¨ˆæ¨æ–‡é¡å‹
                    if tag == 'æ¨':
                        push_count += 1
                    elif tag == 'å™“':
                        boo_count += 1
                    else:
                        neutral_count += 1
            except Exception:
                continue
        
        # æå–æ–‡ç« å…§å®¹
        content_strings = []
        for string in main_content.stripped_strings:
            if (string.startswith('â€»') or 
                string.startswith('â—†') or 
                string.startswith('--')):
                continue
            content_strings.append(string.strip())
        
        content = ' '.join(content_strings)
        content = re.sub(r'\s+', ' ', content).strip()
        
        # æå–ç™¼æ–‡è€… IP
        ip = "Unknown"
        try:
            for string in main_content.strings:
                if 'â€» ç™¼ä¿¡ç«™:' in string:
                    ip_match = re.search(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', string)
                    if ip_match:
                        ip = ip_match.group()
                        break
        except:
            pass
        
        return {
            'title': title,
            'author': author,
            'date': date,
            'content': content,
            'ip': ip,
            'push_count': push_count,
            'boo_count': boo_count,
            'neutral_count': neutral_count,
            'total_messages': push_count + boo_count + neutral_count,
            'messages': messages
        }
    
    def crawl_single_article(self, board_name: str, article_id: str) -> Optional[Article]:
        """çˆ¬å–å–®ç¯‡æ–‡ç« """
        if article_id.endswith('.html'):
            url = f"https://www.ptt.cc/bbs/{board_name}/{article_id}"
        else:
            url = f"https://www.ptt.cc/bbs/{board_name}/{article_id}.html"
        
        self.logger.info(f"æ­£åœ¨çˆ¬å–æ–‡ç« : {url}")
        
        # é¦–å…ˆç²å–åŸºæœ¬ä¿¡æ¯
        basic_info = {
            'board': board_name,
            'article_id': article_id,
            'url': url
        }
        
        # ç²å–è©³ç´°å…§å®¹
        detailed_content = self.parse_article_content(url)
        
        if detailed_content:
            # åˆä½µä¿¡æ¯å‰µå»º Article å°è±¡
            article_data = {**basic_info, **detailed_content}
            article = Article(**article_data)
            return article
        
        return None
    
    def crawl_pages_range(self, board_name: str, start_page: int, end_page: int, 
                         include_content: bool = True) -> List[Article]:
        """çˆ¬å–é é¢ç¯„åœ"""
        self.logger.info(f"é–‹å§‹çˆ¬å– {board_name} çœ‹æ¿ï¼Œé é¢ {start_page} åˆ° {end_page}")
        
        all_articles = []
        total_pages = end_page - start_page + 1
        
        # ä½¿ç”¨é€²åº¦æ¢
        page_iterator = range(start_page, end_page + 1)
        if HAS_TQDM:
            page_iterator = tqdm(page_iterator, desc="çˆ¬å–é é¢")
        
        for page in page_iterator:
            if HAS_TQDM:
                page_iterator.set_description(f"çˆ¬å–ç¬¬ {page} é ")
            else:
                self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
            
            articles_basic = self.extract_articles_from_page(board_name, page)
            
            if include_content:
                # ä¸¦ç™¼çˆ¬å–æ–‡ç« å…§å®¹
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                    future_to_article = {
                        executor.submit(self.parse_article_content, article['url']): article
                        for article in articles_basic
                    }
                    
                    for future in concurrent.futures.as_completed(future_to_article):
                        basic_info = future_to_article[future]
                        try:
                            detailed_content = future.result()
                            if detailed_content:
                                article_data = {**basic_info, **detailed_content}
                                article = Article(**article_data)
                                all_articles.append(article)
                        except Exception as exc:
                            self.logger.error(f"æ–‡ç«  {basic_info['url']} çˆ¬å–å¤±æ•—: {exc}")
            else:
                # åªç²å–åŸºæœ¬ä¿¡æ¯
                for basic_info in articles_basic:
                    article = Article(
                        board=basic_info['board'],
                        article_id=basic_info['article_id'],
                        title=basic_info['title'],
                        author=basic_info['author'],
                        date=basic_info['date'],
                        url=basic_info['url'],
                        content=""
                    )
                    all_articles.append(article)
            
            time.sleep(self.config.delay_between_pages)
        
        self.logger.info(f"çˆ¬å–å®Œæˆï¼å…±ç²å¾— {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def search_articles(self, board_name: str, keyword: str, max_pages: int = 5) -> List[Dict]:
        """æœå°‹åŒ…å«é—œéµå­—çš„æ–‡ç« """
        self.logger.info(f"åœ¨ {board_name} çœ‹æ¿æœå°‹é—œéµå­—: {keyword}")
        
        latest_page = self.get_latest_page_number(board_name)
        start_page = max(1, latest_page - max_pages + 1)
        
        found_articles = []
        
        for page in range(start_page, latest_page + 1):
            articles = self.extract_articles_from_page(board_name, page)
            
            for article in articles:
                if keyword.lower() in article['title'].lower():
                    found_articles.append(article)
                    self.logger.info(f"æ‰¾åˆ°: {article['title']}")
            
            time.sleep(self.config.delay_between_requests)
        
        self.logger.info(f"æœå°‹å®Œæˆï¼Œå…±æ‰¾åˆ° {len(found_articles)} ç¯‡ç›¸é—œæ–‡ç« ")
        return found_articles
    
    def save_articles(self, articles: List[Article], filename: str = None) -> str:
        """ä¿å­˜æ–‡ç« åˆ° JSON æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"articles_{timestamp}.json"
        
        filepath = Path(self.config.output_dir) / filename
        
        data = {
            'articles': [asdict(article) for article in articles],
            'crawl_time': datetime.now().isoformat(),
            'total_articles': len(articles),
            'config': asdict(self.config)
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ–‡ç« å·²ä¿å­˜è‡³: {filepath}")
        return str(filepath)
    
    def export_to_csv(self, articles: List[Article], filename: str = None) -> str:
        """å°å‡ºç‚º CSV æ ¼å¼"""
        if not HAS_PANDAS:
            raise ImportError("éœ€è¦å®‰è£ pandas: pip install pandas")
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"articles_{timestamp}.csv"
        
        filepath = Path(self.config.output_dir) / filename
        
        # æº–å‚™æ•¸æ“š
        csv_data = []
        for article in articles:
            csv_data.append({
                'board': article.board,
                'article_id': article.article_id,
                'title': article.title,
                'author': article.author,
                'date': article.date,
                'content': article.content,
                'ip': article.ip,
                'push_count': article.push_count,
                'boo_count': article.boo_count,
                'neutral_count': article.neutral_count,
                'total_messages': article.total_messages,
                'url': article.url,
                'crawl_time': article.crawl_time
            })
        
        df = pd.DataFrame(csv_data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        self.logger.info(f"CSV æ–‡ä»¶å·²ä¿å­˜è‡³: {filepath}")
        return str(filepath)

class PTTCrawlerCLI:
    """å‘½ä»¤è¡Œç•Œé¢"""
    
    def __init__(self):
        self.config = CrawlConfig()
        self.crawler = PTTCrawler(self.config)
    
    def show_menu(self):
        """é¡¯ç¤ºä¸»é¸å–®"""
        print("\n" + "="*50)
        print("PTT çˆ¬èŸ²å·¥å…·")
        print("="*50)
        print("1. æŸ¥çœ‹çœ‹æ¿æœ€æ–°é æ•¸å’Œæ–‡ç« é è¦½")
        print("2. çˆ¬å–æŒ‡å®šé é¢ç¯„åœ (å®Œæ•´å…§å®¹)")
        print("3. çˆ¬å–å–®ç¯‡æ–‡ç« ")
        print("4. é—œéµå­—æœå°‹æ–‡ç« ")
        print("5. é¡¯ç¤ºæŒ‡å®šé é¢çš„æ–‡ç« åˆ—è¡¨")
        print("6. å°å‡ºå·²çˆ¬å–çš„ JSON ç‚º CSV")
        print("7. é…ç½®è¨­å®š")
        print("8. æ‰¹é‡çˆ¬å–æœ€æ–°æ–‡ç« ")
        print("9. é€€å‡º")
        print("="*50)
    
    def handle_menu_choice(self, choice: str) -> bool:
        """è™•ç†é¸å–®é¸æ“‡"""
        try:
            if choice == '1':
                self._show_board_preview()
            elif choice == '2':
                self._crawl_pages_range()
            elif choice == '3':
                self._crawl_single_article()
            elif choice == '4':
                self._search_articles()
            elif choice == '5':
                self._show_page_articles()
            elif choice == '6':
                self._convert_json_to_csv()
            elif choice == '7':
                self._configure_settings()
            elif choice == '8':
                self._batch_crawl_latest()
            elif choice == '9':
                print("ğŸ‘‹ è¬è¬ä½¿ç”¨ï¼")
                return False
            else:
                print("âŒ ç„¡æ•ˆçš„é¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
        except KeyboardInterrupt:
            print("\nâ¹ï¸  æ“ä½œå·²å–æ¶ˆ")
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return True
    
    def _show_board_preview(self):
        """é¡¯ç¤ºçœ‹æ¿é è¦½"""
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        
        latest_page = self.crawler.get_latest_page_number(board_name)
        if latest_page <= 0:
            print("âŒ ç„¡æ³•ç²å–çœ‹æ¿ä¿¡æ¯")
            return
        
        print(f"\nğŸ“Š {board_name} çœ‹æ¿ä¿¡æ¯:")
        print(f"æœ€æ–°é æ•¸: {latest_page}")
        
        articles = self.crawler.extract_articles_from_page(board_name, latest_page)
        
        if articles:
            print(f"\nğŸ“„ æœ€æ–°é é¢æ–‡ç« é è¦½ (å…± {len(articles)} ç¯‡):")
            print("-" * 80)
            for i, article in enumerate(articles[:10], 1):
                print(f"{i:2d}. ID: {article['article_id']}")
                print(f"    ğŸ“ {article['title']}")
                print(f"    ğŸ‘¤ {article['author']} | ğŸ“… {article['date']} | ğŸ‘ {article['push_preview']}")
                print("-" * 80)
        
    def _crawl_pages_range(self):
        """çˆ¬å–é é¢ç¯„åœ"""
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        
        try:
            start_page = int(input("è«‹è¼¸å…¥èµ·å§‹é æ•¸: "))
            end_page = int(input("è«‹è¼¸å…¥çµæŸé æ•¸: "))
            
            if start_page > end_page:
                print("âŒ èµ·å§‹é æ•¸ä¸èƒ½å¤§æ–¼çµæŸé æ•¸")
                return
            
            include_content = input("æ˜¯å¦åŒ…å«å®Œæ•´å…§å®¹? (y/n): ").lower() == 'y'
            
            articles = self.crawler.crawl_pages_range(board_name, start_page, end_page, include_content)
            
            if articles:
                filename = f"{board_name}-{start_page}-{end_page}.json"
                filepath = self.crawler.save_articles(articles, filename)
                print(f"âœ… æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
                
                if input("æ˜¯å¦ä¹Ÿå°å‡ºç‚º CSV? (y/n): ").lower() == 'y':
                    csv_file = filename.replace('.json', '.csv')
                    self.crawler.export_to_csv(articles, csv_file)
        
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
    
    def _crawl_single_article(self):
        """çˆ¬å–å–®ç¯‡æ–‡ç« """
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        article_id = input("è«‹è¼¸å…¥æ–‡ç« ID: ").strip()
        
        article = self.crawler.crawl_single_article(board_name, article_id)
        
        if article:
            filename = f"{board_name}-{article_id}.json"
            self.crawler.save_articles([article], filename)
            print(f"âœ… æ–‡ç« çˆ¬å–æˆåŠŸ")
            print(f"ğŸ“ æ¨™é¡Œ: {article.title}")
            print(f"ğŸ‘¤ ä½œè€…: {article.author}")
            print(f"ğŸ‘ æ¨æ–‡çµ±è¨ˆ: æ¨{article.push_count} å™“{article.boo_count} ä¸­æ€§{article.neutral_count}")
        else:
            print("âŒ æ–‡ç« çˆ¬å–å¤±æ•—")
    
    def _search_articles(self):
        """æœå°‹æ–‡ç« """
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        keyword = input("è«‹è¼¸å…¥é—œéµå­—: ").strip()
        max_pages = int(input("æœå°‹æœ€è¿‘å¹¾é ? (é è¨­5): ") or "5")
        
        found_articles = self.crawler.search_articles(board_name, keyword, max_pages)
        
        if found_articles:
            print(f"\nğŸ” æ‰¾åˆ° {len(found_articles)} ç¯‡ç›¸é—œæ–‡ç« :")
            for i, article in enumerate(found_articles, 1):
                print(f"{i}. {article['title']} - {article['author']}")
                print(f"   ID: {article['article_id']}")
    
    def _show_page_articles(self):
        """é¡¯ç¤ºé é¢æ–‡ç« """
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        try:
            page_num = int(input("è«‹è¼¸å…¥é æ•¸: "))
            articles = self.crawler.extract_articles_from_page(board_name, page_num)
            
            if articles:
                print(f"\nğŸ“„ {board_name} çœ‹æ¿ç¬¬ {page_num} é  (å…± {len(articles)} ç¯‡):")
                print("-" * 80)
                for i, article in enumerate(articles, 1):
                    print(f"{i:2d}. ID: {article['article_id']}")
                    print(f"    ğŸ“ {article['title']}")
                    print(f"    ğŸ‘¤ {article['author']} | ğŸ“… {article['date']}")
                    print("-" * 80)
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„é æ•¸")
    
    def _convert_json_to_csv(self):
        """è½‰æ› JSON åˆ° CSV"""
        if not HAS_PANDAS:
            print("âŒ éœ€è¦å®‰è£ pandas: pip install pandas")
            return
        
        json_file = input("è«‹è¼¸å…¥ JSON æ–‡ä»¶è·¯å¾‘: ").strip()
        
        if not os.path.exists(json_file):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            articles_data = data.get('articles', [])
            articles = [Article(**article_data) for article_data in articles_data]
            
            csv_file = json_file.replace('.json', '.csv')
            self.crawler.export_to_csv(articles, csv_file)
            
        except Exception as e:
            print(f"âŒ è½‰æ›å¤±æ•—: {e}")
    
    def _configure_settings(self):
        """é…ç½®è¨­å®š"""
        print("\nâš™ï¸  ç•¶å‰è¨­å®š:")
        print(f"è«‹æ±‚é–“éš”: {self.config.delay_between_requests} ç§’")
        print(f"é é¢é–“éš”: {self.config.delay_between_pages} ç§’")
        print(f"ä¸¦ç™¼æ•¸: {self.config.max_workers}")
        print(f"é‡è©¦æ¬¡æ•¸: {self.config.max_retries}")
        print(f"è¼¸å‡ºç›®éŒ„: {self.config.output_dir}")
        
        if input("\næ˜¯å¦ä¿®æ”¹è¨­å®š? (y/n): ").lower() == 'y':
            try:
                self.config.delay_between_requests = float(
                    input(f"è«‹æ±‚é–“éš” ({self.config.delay_between_requests}): ") 
                    or self.config.delay_between_requests
                )
                self.config.delay_between_pages = float(
                    input(f"é é¢é–“éš” ({self.config.delay_between_pages}): ") 
                    or self.config.delay_between_pages
                )
                self.config.max_workers = int(
                    input(f"ä¸¦ç™¼æ•¸ ({self.config.max_workers}): ") 
                    or self.config.max_workers
                )
                
                # é‡æ–°å‰µå»º crawler
                self.crawler = PTTCrawler(self.config)
                print("âœ… è¨­å®šå·²æ›´æ–°")
                
            except ValueError:
                print("âŒ è¼¸å…¥æ ¼å¼éŒ¯èª¤")
    
    def _batch_crawl_latest(self):
        """æ‰¹é‡çˆ¬å–æœ€æ–°æ–‡ç« """
        board_name = input("è«‹è¼¸å…¥çœ‹æ¿åç¨±: ").strip()
        num_pages = int(input("çˆ¬å–æœ€æ–°å¹¾é ? (é è¨­3): ") or "3")
        
        latest_page = self.crawler.get_latest_page_number(board_name)
        start_page = max(1, latest_page - num_pages + 1)
        
        print(f"å°‡çˆ¬å–ç¬¬ {start_page} åˆ° {latest_page} é ")
        
        if input("ç¢ºèªåŸ·è¡Œ? (y/n): ").lower() == 'y':
            articles = self.crawler.crawl_pages_range(board_name, start_page, latest_page)
            
            if articles:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{board_name}_latest_{timestamp}.json"
                self.crawler.save_articles(articles, filename)
                
                print(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼Œå…± {len(articles)} ç¯‡æ–‡ç« ")
    
    def run(self):
        """é‹è¡Œå‘½ä»¤è¡Œç•Œé¢"""
        print("ğŸš€ æ­¡è¿ä½¿ç”¨ PTT çˆ¬èŸ²å·¥å…·!")
        
        while True:
            try:
                self.show_menu()
                choice = input("è«‹é¸æ“‡åŠŸèƒ½ (1-9): ").strip()
                
                if not self.handle_menu_choice(choice):
                    break
                    
                input("\næŒ‰ Enter ç¹¼çºŒ...")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è¦‹!")
                break

def main():
    """ä¸»å‡½æ•¸ - æ”¯æŒå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='PTT çˆ¬èŸ²å·¥å…·')
    parser.add_argument('--board', help='çœ‹æ¿åç¨±')
    parser.add_argument('--pages', help='é é¢ç¯„åœ (æ ¼å¼: start-end)')
    parser.add_argument('--article', help='å–®ç¯‡æ–‡ç« ID')
    parser.add_argument('--search', help='æœå°‹é—œéµå­—')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output', help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    # è¼‰å…¥é…ç½®
    config = CrawlConfig()
    if args.config and os.path.exists(args.config):
        config = CrawlConfig.from_file(args.config)
    
    if args.output:
        config.output_dir = args.output
    
    crawler = PTTCrawler(config)
    
    # å‘½ä»¤è¡Œæ¨¡å¼
    if args.board:
        if args.pages:
            start, end = map(int, args.pages.split('-'))
            articles = crawler.crawl_pages_range(args.board, start, end)
            if articles:
                filename = f"{args.board}-{start}-{end}.json"
                crawler.save_articles(articles, filename)
        
        elif args.article:
            article = crawler.crawl_single_article(args.board, args.article)
            if article:
                crawler.save_articles([article], f"{args.board}-{args.article}.json")
        
        elif args.search:
            found = crawler.search_articles(args.board, args.search)
            for article in found:
                print(f"{article['title']} - {article['author']}")
    
    else:
        # äº¤äº’æ¨¡å¼
        cli = PTTCrawlerCLI()
        cli.run()

if __name__ == "__main__":
    main()
